This si a comment sumarization for the UH course evaluations.

It takes the resposes from an excel file as it is in testing phase and try to get the best summary out using the bert scores.

In this project we have used 
## 1) T5
T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format.
## 2) GPT2
## 3) GPT3 

## 4) GPT-Neo
The architecture is similar to GPT2 except that GPT Neo uses local attention in every other layer with a window size of 256 tokens.
## 5) llama3 using ollama
